---
layout: splash
title: "Sentiment Analysis on the Literary Works of Oscar Wilde"
author: "Victor Murcia"
subtitle: "The truth is rarely pure and never simple."
date: 2022-08-24 08:33:00 -0400
tags: jupyter_notebook  data_science natural_language_processing featured portfolio
background: '/img/posts/lit.webp'
---


<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<p>Oscar Wilde is one of my favourite authors ever. 'The Picture of Dorian Gray ' published in 1890, remains as one of the stories that has resonated and impacted me the most. The idea of a painting granting you a carte blanche and being the receptacle of ones evil while you live a life devoid of consequences was fascinating to me. In particular, the gradual deterioration of the painting as Dorian Gray committed increasingly terrible acts will always be one of my favorite concepts in literature. </p>
<p style="text-align:center;"><img src="\img\posts\Oscar Wilde\dorian1.png" width = 800 height = 1000></p>

<p>As such, it's only natural for me to want to investigate them through the lenses of data science and natural language processing (NLP) techniques.</p>

<p>NLP is an amalgam of the concepts and techniques from the fields of linguistics, computer science, and artificial intelligence that seeks to understand how computers process human language. Some common tasks used by NLP are text and speech processing used in Speech recognition applications (i.e., Siri from Apple and Alexa from Amazon) and Higher-level applications like Discourse management (i.e., how can a computer converse with a human) amongst many others. It is an absolutely fascinating field and one that I've been deeply interested in from the moment I learned about it.</p>

<p>Some questions I'd want to answer are: </p>

<ul>
    <li>What are the most commonly used words used by Oscar Wilde?</li>
    <li>What is the overall sentiment present in his works?</li>
    <li>How does the sentiment change (if at all) from chapter to chapter? Is there a particular emotional structure that is present?</li>
    <li>Are the emotional assessments in agreement with the themes present in the narratives?</li>
</ul>

<p>Project Gutenberg is a library of over 60,000 free eBooks that one can download or read online. Oscar Wilde is one of the many authors that Project Gutemberg has works readily available for.</p>
<p style="text-align:center;"><img src="\img\posts\Oscar Wilde\pg.jpg" width = 800 height = 1000></p>
<p>I'll write some code to download his works from <a href = "https://www.gutenberg.org/">Project Gutenberg</a>. These routines should be readily applicable to get stuff from other authors too!</p>

<p>This project will show more of my code directly since I will be doing a bit of querying and webscraping to get the data I need which I think is 
    worth showing. If you are interested in seeing the full set of code you can see my post <a href = "https://victormurcia.github.io/Sentiment-Analysis-on-the-Works-of-Oscar-Wilde/">here</a> or my repository <a href = "https://github.com/victormurcia/Sentiment-Analysis-on-the-Literary-Works-of-Oscar-Wilde">here</a>.</p>

<p>In terms of the sentiment/emotion analysis, I'll start by using a Naive Bayes classifier trained on Twitter data to determine whether the sentiments in a chapter are 
positive, negative or neutral. Then, I'll use VADER to get polarity scores. Finally, I'll use the NRC lexicon to breakdown each chapter into 10 different 
emotions (joy, positive, anger, disgust, negative, anticipation, surprise, trust, fear, and surprise).</p>

<p>With that said let's dive in!</p>

<h2>Prelude and Exposition - Structuring the Query for Oscar Wilde in Project Gutemberg</h2>

<p>The first thing I'll need is a way to retrieve the contents from the HTML site of Project Gutenberg. This means I'll need to communicate with the site/server which I can readily do with the <code>urlopen</code> function. The URL that I'm trying to connect to is associated with a search query for 'Oscar Wilde' and it looks as follows:</p>

<p><code>https://www.gutenberg.org/ebooks/search/?query=oscar+wilde&submit_search=Search</code></p>
    
<p>I'll be replacing the string to make it more general by using a formatted string of the form</p>
    
<p><code>f'https://www.gutenberg.org/ebooks/search/?query={first_name}+{last_name}&submit_search=Search'</code></p>
    
<p>I should note that Project Gutenberg queries are limited to 25 search results but that should be enough for this initial exploration.  </p>

<p>Now that I know what URL to query, I can start to build the functions required to get the info I need from Project Gutemberg.</p>

<h3>Step 1 - Communicating With the Query URL</h3>
<p>Now that I know the structure of the URL that I need to query I can write a function that will communicate with the site as shown below:</p>
<script src="https://gist.github.com/victormurcia/fde61cce600282690e30b90efd4f5e35.js"></script>

<h3>Step 2 - Webscraping the Query URL for ALL URLs in page</h3>
<p>Next, I need a way to retrieve the URLs associated with book files from the search query. BeautifulSoup allows us to do this easily since all book links are contained within the <code>'a href'</code> tag. Hence, all that needs to be done is tell BeautifulSoup to find all the tags that begin with 'a' and then get the parts that contain an actual link that's denoted by 'href'. The function below takes care of that. </p>
<script src="https://gist.github.com/victormurcia/5121186f1c59f10bca2c3c0ed816f1d0.js"></script>

<h3>Step 3 - Get Book Identifiers</h3>
<p>Now that we have the links present in the search query, I need to get the book identifiers that Project Gutenberg uses. This is done via the function below which will return a list of bookIDs that we can then use to download all the books associated with the results of our search query.</p>
<script src="https://gist.github.com/victormurcia/b47d506791efc576dc16c0ff344b92ae.js"></script>

<h3>Step 4 - Downloading A Single Book from Project Gutemberg</h3>
<p>We are almost done with the extraction prep for our books! Now that I have the bookIDs associated with the query results I can write a function that will download the book onto a directory in our computer. This is done with the function below. </p>
<script src="https://gist.github.com/victormurcia/2f463960edc66654daaf605105bc4d80.js"></script>

<p>Let's test it out! The book ID for 'The Picture Of Dorian Gray' is 174. So I'll just pass that into my function and the result is a nice 
    file in my directory called '174.txt' that contains 'The Portrait Of Dorian Gray'. Awesome!</p>

<p style="text-align:center;"><img src="\img\posts\Oscar Wilde\dl1.png" width = 800 height = 1000></p>

<p>Perfect! Now that I have my base download function I can simply put my routines into a wrapper function that will allow me to download all the books present in our search query page as shown below.</p>

<h2>Rising Action - Downloading Oscar Wilde's Literary Works from Project Gutemberg</h2>
<p>Here is the wrapper function that will allow me to download all the books in the Oscar Wilde search query page.</p>
<script src="https://gist.github.com/victormurcia/ea58bbd3321a8efb0b2dfd08b45f0a67.js"></script>
<p>The results of this running this function are shown below.</p>

<p style="text-align:center;"><img src="\img\posts\Oscar Wilde\all_books.jpg" width = 800 height = 1000></p>
<p>Our function returned 23 out of the 25 results that were on the search query page. The 2 results that were omitted corresponded to an audio book for the 'The Importance of Being Earnest' and the German version of 'The Portrait of Dorian Gray' which is titled 'Das Bildnis des Dorian Gray'. This is totally okay though since for my work I'm only interested in text data and documents that are in English so I'll proceed with the next stage of the process.</p>

<h3>Breaking the Book into Chapters</h3>
<p>Now that I have the books downloaded, I'm going to write a routine that will split each book into Chapters. This will allow me to process and analyze each 
    book chapter individually.</p>
 
<script src="https://gist.github.com/victormurcia/5fc5653ea06fb119a61e083c8046f248.js"></script>
<p>Next I'll load the book chapter files that we have generated to have them accessible</p>
<script src="https://gist.github.com/victormurcia/249f2941fbc7d3ade7db5dfe31d3f8c5.js"></script>

<p>Finally, I'll  read the contents of the book and place them in memory here. I'll start with Chapter 1 which was saved as a file named '2.txt' since the current book has a Prelude Chapter</p>
<script src="https://gist.github.com/victormurcia/b9f9d91c7962219efe18aab865de0169.js"></script>
<p>The output of this routine is shown below:</p>
<p style="text-align:center;"><img src="\img\posts\Oscar Wilde\ch1.JPG" width = 600 height = 800></p>
<p>We are ready to roll!</p>

<h2>The Midpoint - Tokenizing, Lemmatizing, and Denoising A Chapter</h2>
<p>I now have the entire book at my disposal to work with. However, there's a bit of preparation I need to do on the text data before I can carry out 
    the sentiment analysis. Before I move on to processing an entire book, I want to ensure that all my routines will work as intended on a chapter. This is how I like to code, learn, design experiments and do research in general. Start with a simple foundation that I can build as much or as little detail and complexity as needed.</p>
<p>I'll start with Chapter 1 from 'The Picture of Dorian Gray'.</p>

<h3>Tokenizing The Chapter</h3>
<p>Tokenizing is the process of spliting strings into smaller parts called tokens. A token is the smallest unit of text that can be parsed by the computer (analogues to phonemes and morphemes in human languages). Tokens can be any sequence of words, emojis, hashtags, links, or even an individual character. It is an important part of the preparation of sentiment analysis since it provides the framework from which the computer will try to ascribe meaning.</p>
<p>Let's start by tokenizing the chapter. I'll use the <code>word_tokenize</code> function from nltk. I'll also remove the first three elements from the tokenized list since those are just the chapter numbers and empty lines.</p>
<script src="https://gist.github.com/victormurcia/217e740f539ef46c9e0f71043fdd40fb.js"></script>
<p>The output of this process is shown below where it can be seen that the tokenizer generated an array of words from the chapter:</p>
<p style="text-align:center;"><img src="\img\posts\Oscar Wilde\token1.JPG" width = 800 height = 1000></p>

<h3>Normalizing the Words in a Chapter Via Lemmatization</h3>
<p>Words can be expressed in a variety of ways to convey meaning. For instance, ate and eaten are different tenses of the verb eat. An analysis may require these different tenses to be converted into their base, fundamental, canonical form. This process is known as normalization and is commonly achieved via stemming and/or lemmatization algorithms.</p>
<p>In lemmatization, the algorithm normalizes a word within the context of vocabulary and through the use of morphological analysis in order to produce a lemma. A lemma in morphology is known as the canonical or dictionary form of a set of word forms. This lemmatization process allows different words to be indexed,mapped, or traced back to a single word. For instance, speak, speaks, spoke, spoken and speaking are all forms of speak (known as the lexeme).</p>
<p>Through lemmatization, I'll be able to tag words and denote them as nouns, verbs, prepositions, and others. This can be readily done with nltk as shown below</p>
<script src="https://gist.github.com/victormurcia/185870999701b0821e6a621c9af37ffb.js"></script>
<p>The output of this is shown below:</p>
<p style="text-align:center;"><img src="\img\posts\Oscar Wilde\lemmatization1.JPG" width = 800 height = 1000></p>
<p>That worked! Every tokenized word has a tag associated with it now.  Notice how there is a comma token after 'rose'. I'll remove that and other things as part of the denoising/cleanup process.</p>

<h3>Denoising a Chapter</h3>
<p>I'm almost done preparing the chapter to carry out sentiment analysis. Before I do so however, I need to remove punctuations, stop words, and special characters that are muddying the data before continuing. This process is known as denoising.</p>
<p>To denoise data, I'll be using regular expressions (regexs) through the function below:</p>
<script src="https://gist.github.com/victormurcia/6bd5646c69b7c146d5a7cf5f185d5435.js"></script>
<p>The result of running this function on the chapter is shown below:</p>
<p style="text-align:center;"><img src="\img\posts\Oscar Wilde\denoise1.JPG" width = 800 height = 1000></p>
<p>Lovely! Now we are ready to start doing analysis on this chapter!</p>

<h3>Calculating Word Density for Chapter</h3>
<p>Now that the chapter is tokenized, lemmatized and denoised, let me find out what are the top 20 most frequently occurring words in the chapter.</p>
<p style="text-align:center;"><img src="\img\posts\Oscar Wilde\wf_ch1.jpg" width = 800 height = 1000></p>
<p>Looks like the most common word in this chapter is 'say' with 29 instances, followed by the word 'one' with 27 instances. </p>

<h2>The Crisis - Naive Bayes Classifier</h2>
<p>Now let's train a model! I'll start by using a Naive Bayes Classifier based on the twitter_samples dataset and see how that works out. I've covered this in a previous post so I'll just include the results here. This will allow me to readily classify words as being associated with positive or negative  sentiments. I will later try to classify the words, sentences, chapters and entire book with more detailed emotions like joy, sadness, anger, and such.  </p>
<p>The results of the classifier are shown below:</p>
<p style="text-align:center;"><img src="\img\posts\Oscar Wilde\nb_class.JPG" width = 400 height = 400></p>
<p>The classifier has a 99.2% classification accuracy. Let's run it on the model!</p>
<p>I'll prepare the chapter to be classified via the Naive Bayes, which requires me to convert the denoised chapter into a dictionary where each word 
    will be paired with a True/False value using the function below.</p>
<script src="https://gist.github.com/victormurcia/ea9d81aab50ea704560b14555da4539e.js"></script>
<p>Then, I can run the classifier on the chapter dictionary I just made and the output will be whether the chapter is considered to be 'Positive' or 'Negative'</p>
<script src="https://gist.github.com/victormurcia/3448058da259b1c54e61812c46107ab9.js"></script>
<p>The output of running the Naive Bayes classifier for this chapter turned out to be 'Negative'. This is a sentiment that I think I can agree with, though there 
    is a lot more to be said about the emotions within this chapter. Regardless, now I'm ready to apply the routines developed so far to the entire book!</p>

<h2>The Big Event - Tokenizing, Lemmatizing, and Denoising A Book</h2>
<p>Now I'm ready to process the entire book and determine the dominating sentiment present in each chapter using the wrapper function below.</p>
<script src="https://gist.github.com/victormurcia/ffc243b0c31d53b8af28b101029bda5c.js"></script>
<p>The results of running this routine are shown below:</p>
<p style="text-align:center;"><img src="\img\posts\Oscar Wilde\nb_results.JPG" width = 400 height = 400></p>
<p>Yikes! The classifier determined that every chapter in the book is negative... Though I'm inclined to agree with that sentiment, it is a pretty boring result that oversimplifies the variety of nuances that make the book interesting. As such, I decided to look into the VADER and NRC lexicons to get additional color into the emotions present in the chapter.</p>

<h2>The Climax - VADER and NRC Lexicon</h2>
<p>VADER will give polarity scores. Even though the sentiment categorization is still set to a simple positive, negative, or neutral, we will get a quantifiable metric for each of those values and start getting a better idea of the sentiment composition from each chapter. I'll first modify my wrapper routine from above so that it returns the chapter contents as part of the output.</p>
<p>Next, I'll create a pandas dataframe that will hold the contents from each chapter on each row using the code below.</p>
<script src="https://gist.github.com/victormurcia/9665f8f55b27a9d20d514913ee5da6de.js"></script>
<p>The resulting dataframe for the book is shown below:</p>
<p style="text-align:center;"><img src="\img\posts\Oscar Wilde\df_book.JPG" width = 400 height = 400></p>
<p>And now I'll run the SentimentIntensityAnalyzer with VADER and save the polarity scores for each chapter into the dataframe using the code below:</p>
<script src="https://gist.github.com/victormurcia/240cb58522748d81741d5c9ea90f6605.js"></script>
<p>The resulting dataframe is shown below:</p>
<p style="text-align:center;"><img src="\img\posts\Oscar Wilde\df_vader.JPG" width = 600 height = 600></p>
<p>Okay, now things are starting to get a bit more interesting. Each chapter has different positive, negative and neutral scores associated with them. Furthermore, the compound scores (which represent the overall sentiment with +1 being the most positive and -1 being the most negative) are also different between chapters. Let me plot them and see how it looks like!</p>
<p style="text-align:center;"><img src="\img\posts\Oscar Wilde\vader.jpg" width = 600 height = 600></p>




<p style="text-align:center;"><img src="\img\posts\Oscar Wilde\cmat_dg.png" width = 800 height = 1000></p>


<h2>The Realization - Visualization of the Emotional Analysis</h2>
<p style="text-align:center;"><img src="\img\posts\Oscar Wilde\wordcloud_dg.png" width = 800 height = 1000></p>

<h2>Conclusion</h2>
<p>Sentiment Analysis and Emotion Recognition was used to analyze 'The Portrait of Dorian Gray' by Oscar Wilde. The results of the routine showcased here elucidated some of the thematic/emotional structure of that book using a Naive Bayes classifier, VADER and NRC lexicons for every chapter in the book. The next steps for this project involve processing the remaining works from Oscar Wilde and seeing if there is a common emotional structure in his books.</p>